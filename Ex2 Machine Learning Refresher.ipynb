{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NN_01_MiniTask.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP9J9JWm8g0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Group name: berlinmerkez\n",
        "#Group members: Atacan Korkmaz, Harika Duyu, Kaan Isik\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import preprocessing\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras import optimizers\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PONWgRi1k7OA",
        "colab_type": "text"
      },
      "source": [
        "First, we read the data and check the number of 0s and 1s in the target variable 'Kauf'. We can see that we are working with an imbalanced data. Even though deep neural network models are considered to be robust against the imbalances in the data, we'll be eliminating the imbalance in the data as we tested the model with both imbalanced and balanced data and got better results using the latter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZkjjlM19gtz",
        "colab_type": "code",
        "outputId": "51003d91-c0d6-4b33-c277-ec2e14e019e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "link = \"https://drive.google.com/file/d/1zSlMX4ou0q7tfvtzI4uOAf_RHk1JCltZ/view?usp=sharing\"\n",
        "plop, id = link.split('d/')\n",
        "id, plop = id.split('/view')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile(\"ConversionDataSet.csv\")  \n",
        "df = pd.read_csv(\"ConversionDataSet.csv\")\n",
        "df.head()\n",
        "\n",
        "target_count = df.Kauf.value_counts()\n",
        "print('Class 0:', target_count[0])\n",
        "print('Class 1:', target_count[1])\n",
        "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
        "target_count.plot(kind='bar', title='Count (target)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 0: 47237\n",
            "Class 1: 4112\n",
            "Proportion: 11.49 : 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa0bdce0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEFCAYAAAAIZiutAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2bkYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ6Wy_P1u_8q",
        "colab_type": "text"
      },
      "source": [
        "In the dataset, the columns **category**, **browser**, **operatingSystem** contain categorical data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YYErH3g8shU",
        "colab_type": "code",
        "outputId": "b079e101-4874-4d5d-a809-b748b710e498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "categories = df.category.unique()\n",
        "print(f\"categories : {categories}\")\n",
        "print(f\"number of unique categories = {len(categories)}\")\n",
        "\n",
        "browsers = df.browser.unique()\n",
        "print(f\"browsers : {browsers}\")\n",
        "print(f\"number of unique browsers = {len(browsers)}\")\n",
        "\n",
        "OS = df.operatingSystem.unique()\n",
        "print(f\"Operating Systems : {OS}\")\n",
        "print(f\"number of unique Operating System = {len(OS)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "categories : ['overview' 'search' 'product' 'account' 'sale' 'home' 'about' 'cart'\n",
            " 'checkout' 'other']\n",
            "number of unique categories = 10\n",
            "browsers : ['IE' 'Firefox' 'Chrome' 'Safari' 'Yandex Browser'\n",
            " 'Mail.ru Chromium Browser' 'Pale Moon (Firefox Variant)' 'Iceweasel'\n",
            " 'Maxthon' 'Iron' 'Edge' 'SeaMonkey' 'QQ Browser' 'Lunascape']\n",
            "number of unique browsers = 14\n",
            "Operating Systems : ['Windows 7' 'Windows 8' 'Windows 8.1' 'Windows 10' 'Windows Vista'\n",
            " 'Mac OS X' 'Windows XP' 'Ubuntu' 'Chrome OS' 'Windows RT 8.1' 'Linux'\n",
            " 'Windows RT']\n",
            "number of unique Operating System = 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMPfXn0Pyy9L",
        "colab_type": "code",
        "outputId": "c2d20d3d-933d-44e0-f805-90fa86a1d095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"total number of rows = {len(df)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of rows = 51349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLKPOcwuz39g",
        "colab_type": "text"
      },
      "source": [
        "We will check if there are any missing values in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u71mOFsuzdkJ",
        "colab_type": "code",
        "outputId": "832c2295-8494-472f-e2cc-fad41415d4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "isnull = df.isnull().sum() \n",
        "\n",
        "print(f\"number of columns that contains an empty cell: {len(isnull[isnull>0])}\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of columns that contains an empty cell: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3AeT9JL3C-9",
        "colab_type": "text"
      },
      "source": [
        "So, there are no empty cells we have to deal with. We will dummify the categorical columns that we mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQJas_DM-bwq",
        "colab_type": "code",
        "outputId": "f87e4342-d730-4861-a5da-c28c03983166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "source": [
        "df = pd.get_dummies(df, prefix=['category', 'browser', 'operatingSystem'])\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>month</th>\n",
              "      <th>Kauf</th>\n",
              "      <th>sessionStartHour</th>\n",
              "      <th>dayOfMonth</th>\n",
              "      <th>weekday</th>\n",
              "      <th>sessionTime</th>\n",
              "      <th>pageVisitedBeforeSession</th>\n",
              "      <th>amountBasketSession</th>\n",
              "      <th>totalItemValueBasketSession</th>\n",
              "      <th>sessionProduct</th>\n",
              "      <th>sessionSearch</th>\n",
              "      <th>sessionOverview</th>\n",
              "      <th>sessionSale</th>\n",
              "      <th>sessionCart</th>\n",
              "      <th>percPageProduct</th>\n",
              "      <th>percPageSearch</th>\n",
              "      <th>percPageOverview</th>\n",
              "      <th>percPageSale</th>\n",
              "      <th>percPageCart</th>\n",
              "      <th>countPagesRevisited</th>\n",
              "      <th>timeOnPage</th>\n",
              "      <th>checkoutStep</th>\n",
              "      <th>clickEventsSession</th>\n",
              "      <th>scrollEventSession</th>\n",
              "      <th>tabSwitchSession</th>\n",
              "      <th>clickEventProduct</th>\n",
              "      <th>scrollEventProduct</th>\n",
              "      <th>tabSwitchProduct</th>\n",
              "      <th>clickEventSearch</th>\n",
              "      <th>scrollEventSearch</th>\n",
              "      <th>tabSwitchSearch</th>\n",
              "      <th>clickEventOverview</th>\n",
              "      <th>scrollEventOverview</th>\n",
              "      <th>tabSwitchOverview</th>\n",
              "      <th>clickEventSale</th>\n",
              "      <th>scrollEventSale</th>\n",
              "      <th>tabswitchSale</th>\n",
              "      <th>clickEventCart</th>\n",
              "      <th>scrollEventCart</th>\n",
              "      <th>...</th>\n",
              "      <th>windowWidth</th>\n",
              "      <th>windowHeight</th>\n",
              "      <th>tabVisible</th>\n",
              "      <th>visitorKnown</th>\n",
              "      <th>category_about</th>\n",
              "      <th>category_account</th>\n",
              "      <th>category_cart</th>\n",
              "      <th>category_checkout</th>\n",
              "      <th>category_home</th>\n",
              "      <th>category_other</th>\n",
              "      <th>category_overview</th>\n",
              "      <th>category_product</th>\n",
              "      <th>category_sale</th>\n",
              "      <th>category_search</th>\n",
              "      <th>browser_Chrome</th>\n",
              "      <th>browser_Edge</th>\n",
              "      <th>browser_Firefox</th>\n",
              "      <th>browser_IE</th>\n",
              "      <th>browser_Iceweasel</th>\n",
              "      <th>browser_Iron</th>\n",
              "      <th>browser_Lunascape</th>\n",
              "      <th>browser_Mail.ru Chromium Browser</th>\n",
              "      <th>browser_Maxthon</th>\n",
              "      <th>browser_Pale Moon (Firefox Variant)</th>\n",
              "      <th>browser_QQ Browser</th>\n",
              "      <th>browser_Safari</th>\n",
              "      <th>browser_SeaMonkey</th>\n",
              "      <th>browser_Yandex Browser</th>\n",
              "      <th>operatingSystem_Chrome OS</th>\n",
              "      <th>operatingSystem_Linux</th>\n",
              "      <th>operatingSystem_Mac OS X</th>\n",
              "      <th>operatingSystem_Ubuntu</th>\n",
              "      <th>operatingSystem_Windows 10</th>\n",
              "      <th>operatingSystem_Windows 7</th>\n",
              "      <th>operatingSystem_Windows 8</th>\n",
              "      <th>operatingSystem_Windows 8.1</th>\n",
              "      <th>operatingSystem_Windows RT</th>\n",
              "      <th>operatingSystem_Windows RT 8.1</th>\n",
              "      <th>operatingSystem_Windows Vista</th>\n",
              "      <th>operatingSystem_Windows XP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1280</td>\n",
              "      <td>844</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>85</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.112741</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1339</td>\n",
              "      <td>634</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1600</td>\n",
              "      <td>799</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>162</td>\n",
              "      <td>1</td>\n",
              "      <td>9.95</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.045554</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1280</td>\n",
              "      <td>891</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>39.90</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1600</td>\n",
              "      <td>775</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 130 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  month  ...  operatingSystem_Windows Vista  operatingSystem_Windows XP\n",
              "0           4      8  ...                              0                           0\n",
              "1          14      8  ...                              0                           0\n",
              "2          51      8  ...                              0                           0\n",
              "3          60      8  ...                              0                           0\n",
              "4          70      8  ...                              0                           0\n",
              "\n",
              "[5 rows x 130 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIN-0O2EjXxZ",
        "colab_type": "text"
      },
      "source": [
        "We will prepare the target variable 'Kauf' by keeping it in y and keeping the rest of the data in X. Then, we'll split the data into train and test, using 70% of the data for training. We will scale the data after the train-test split to not introduce future information to out training test. It's important to feed only X_train values to not leak the data to the test set. We will finish the processing of the data by over-sampling the 0s using the SMOTE method to have a balanced dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7qOlNR_jTc",
        "colab_type": "code",
        "outputId": "1d5be52b-1792-4507-c73c-656991868a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "seed = 101\n",
        "X = df.drop([\"Kauf\"], axis=1)\n",
        "y = df.Kauf\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=seed)    \n",
        "print(X_train.shape)\n",
        "print(y_train.shape) \n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train= scaler.transform(X_train)\n",
        "X_test= scaler.transform(X_test)\n",
        "\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train, y_train = sm.fit_sample(X_train, y_train.ravel())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35944, 129)\n",
            "(35944,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQmVzQxvnsAM",
        "colab_type": "text"
      },
      "source": [
        "We will create our model structure and wrap it into a function called create_model. Then, we will plug it into Keras Classifier Wrapper to use them in the scikit-learnlibrary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dAhpAdKNit4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, activation = 'relu', kernel_initializer=keras.initializers.he_normal(seed=seed), \n",
        "                  bias_initializer='zeros', input_shape=(129,)))\n",
        "  model.add(Dense(128, activation = 'relu', kernel_initializer=keras.initializers.he_normal(seed=seed), \n",
        "                  bias_initializer='zeros'))\n",
        "  model.add(Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.he_normal(seed=seed),\n",
        "                  bias_initializer='zeros'))\n",
        "  model.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWmZQRJ2qP2o",
        "colab_type": "text"
      },
      "source": [
        "We defined the grid search parameters for the batch size and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWMtwJhzOnca",
        "colab_type": "code",
        "outputId": "f12b84c8-78ff-4185-da00-bb987c700387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batchsize = [16, 32, 64, 128]\n",
        "epochs = [10,25,50]\n",
        "\n",
        "param_grid = dict(batch_size=batchsize, epochs=epochs)\n",
        "param_grid "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': [16, 32, 64, 128], 'epochs': [10, 25, 50]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS5mousdqvOn",
        "colab_type": "text"
      },
      "source": [
        "We will do a grid search to try every combination of batch size and number of epochs values to find the best scoring duo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy5huK1ZOzqI",
        "colab_type": "code",
        "outputId": "ac3a62d2-6bd2-4a0d-a12a-a624702e7e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=3, n_jobs=-1)\n",
        "results = grid.fit(X_train, y_train)\n",
        "print(\"Best: %f using %s\" % (results.best_score_, results.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.916367 using {'batch_size': 128, 'epochs': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiWgJVs4Fo5V",
        "colab_type": "text"
      },
      "source": [
        "The best scoring duo is found as 64 for the batch size and 50 for the number of epochs. Now, we will fix those parameters and then do a grid search for the best scoring optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLhmNgrYbyfy",
        "colab_type": "code",
        "outputId": "008bbe4b-7865-438c-ea36-bfeddb1084ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def optim (optimizer='SGD'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed),  input_shape=(129,)))\n",
        "    model.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "    model.add(Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "    \n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy']) \n",
        "    \n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=optim, epochs=50, batch_size=64, verbose=0)\n",
        "# define the grid search parameters\n",
        "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizers) # setting a dictionary to store the results\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "results = grid.fit(X_train, y_train)\n",
        "\n",
        "# Our results (reember these are the resultson the hold-out data)\n",
        "print(\"Best: %f using %s\" % (results.best_score_, results.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.939269 using {'optimizer': 'Adadelta'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0RUZg3dF2KK",
        "colab_type": "text"
      },
      "source": [
        "Best scoring optimizer is found as 'Nadam'. Now we will fix that parameter as well and do a grid search to find the best scoring learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhUTgpNQxlcz",
        "colab_type": "code",
        "outputId": "775240cb-a8f9-4f63-9c6b-2529465d12e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def create_model_lr(learn_rate=0.01):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed), input_shape=(129,)))\n",
        "  model.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "  model.add(Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "  optimizer = keras.optimizers.Nadam(lr=learn_rate)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model_lr, epochs=50, batch_size=64, verbose=0)\n",
        "\n",
        "learn_rate = [0.001, 0.01, 0.1]\n",
        "param_grid = dict(learn_rate=learn_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.937275 using {'learn_rate': 0.001}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMnKPLH-GFCA",
        "colab_type": "text"
      },
      "source": [
        "Best scoring learning rate is found as 0.001. Now, we will create our deep neural network model using the best scoring parameters. Best scoring number of hidden layers and number of neurons in a layer are found by manually trying the possibilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ujUI5WHpem",
        "colab_type": "code",
        "outputId": "89f6f25b-82bf-4eef-eac8-cc2c649919a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2000
        }
      },
      "source": [
        "early_stopping_monitor = EarlyStopping(patience=7)\n",
        "batch_size = 64 \n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "#decay_rate = learning_rate / epochs\n",
        "#momentum = 0.8\n",
        "\n",
        "model_final = Sequential()\n",
        "model_final.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed), input_shape=(129,)))\n",
        "model_final.add(Dropout(0.1))\n",
        "model_final.add(Dense(128, activation='relu',bias_initializer='zeros',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "model_final.add(Dropout(0.1))\n",
        "model_final.add(Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.he_normal(seed=seed)))\n",
        "\n",
        "opt = keras.optimizers.Adadelta(lr=learning_rate)\n",
        "model_final.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#And now we fit the model (that is the training part), mind that saving it to the variable will help you retrieve and analyse the training history\n",
        "story = model_final.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1, validation_data=(X_test, y_test), callbacks=[early_stopping_monitor])\n",
        "\n",
        "#score = model.evaluate(X_test, y_test, verbose=0)\n",
        "#print('Test loss:', score[0])\n",
        "#print('Test accuracy:', score[1])\n",
        "\n",
        "def show_history(story):\n",
        "    plt.plot(story.history['acc'])\n",
        "    plt.plot(story.history['val_acc'])\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "show_history(story)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 66194 samples, validate on 15405 samples\n",
            "Epoch 1/50\n",
            "66194/66194 [==============================] - 12s 180us/step - loss: 1.0479 - acc: 0.4592 - val_loss: 0.8201 - val_acc: 0.5598\n",
            "Epoch 2/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.9563 - acc: 0.4777 - val_loss: 0.8355 - val_acc: 0.5390\n",
            "Epoch 3/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.8926 - acc: 0.5055 - val_loss: 0.8373 - val_acc: 0.5306\n",
            "Epoch 4/50\n",
            "66194/66194 [==============================] - 5s 76us/step - loss: 0.8482 - acc: 0.5205 - val_loss: 0.8282 - val_acc: 0.5325\n",
            "Epoch 5/50\n",
            "66194/66194 [==============================] - 5s 74us/step - loss: 0.8141 - acc: 0.5391 - val_loss: 0.8114 - val_acc: 0.5443\n",
            "Epoch 6/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.7916 - acc: 0.5495 - val_loss: 0.7930 - val_acc: 0.5568\n",
            "Epoch 7/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.7733 - acc: 0.5597 - val_loss: 0.7732 - val_acc: 0.5722\n",
            "Epoch 8/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.7602 - acc: 0.5686 - val_loss: 0.7545 - val_acc: 0.5912\n",
            "Epoch 9/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.7437 - acc: 0.5772 - val_loss: 0.7374 - val_acc: 0.6077\n",
            "Epoch 10/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.7341 - acc: 0.5825 - val_loss: 0.7230 - val_acc: 0.6190\n",
            "Epoch 11/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.7248 - acc: 0.5881 - val_loss: 0.7094 - val_acc: 0.6313\n",
            "Epoch 12/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.7174 - acc: 0.5911 - val_loss: 0.6975 - val_acc: 0.6428\n",
            "Epoch 13/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.7128 - acc: 0.5941 - val_loss: 0.6883 - val_acc: 0.6521\n",
            "Epoch 14/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.7041 - acc: 0.5995 - val_loss: 0.6795 - val_acc: 0.6624\n",
            "Epoch 15/50\n",
            "66194/66194 [==============================] - 5s 76us/step - loss: 0.6996 - acc: 0.6025 - val_loss: 0.6709 - val_acc: 0.6705\n",
            "Epoch 16/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.6942 - acc: 0.6086 - val_loss: 0.6651 - val_acc: 0.6769\n",
            "Epoch 17/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6916 - acc: 0.6077 - val_loss: 0.6593 - val_acc: 0.6824\n",
            "Epoch 18/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6892 - acc: 0.6094 - val_loss: 0.6540 - val_acc: 0.6876\n",
            "Epoch 19/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6833 - acc: 0.6135 - val_loss: 0.6497 - val_acc: 0.6929\n",
            "Epoch 20/50\n",
            "66194/66194 [==============================] - 6s 85us/step - loss: 0.6789 - acc: 0.6131 - val_loss: 0.6452 - val_acc: 0.6976\n",
            "Epoch 21/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6756 - acc: 0.6175 - val_loss: 0.6407 - val_acc: 0.7015\n",
            "Epoch 22/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6716 - acc: 0.6193 - val_loss: 0.6369 - val_acc: 0.7050\n",
            "Epoch 23/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.6685 - acc: 0.6212 - val_loss: 0.6338 - val_acc: 0.7068\n",
            "Epoch 24/50\n",
            "66194/66194 [==============================] - 5s 81us/step - loss: 0.6665 - acc: 0.6233 - val_loss: 0.6311 - val_acc: 0.7099\n",
            "Epoch 25/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6633 - acc: 0.6240 - val_loss: 0.6275 - val_acc: 0.7126\n",
            "Epoch 26/50\n",
            "66194/66194 [==============================] - 5s 81us/step - loss: 0.6634 - acc: 0.6256 - val_loss: 0.6237 - val_acc: 0.7159\n",
            "Epoch 27/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6599 - acc: 0.6266 - val_loss: 0.6211 - val_acc: 0.7183\n",
            "Epoch 28/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6588 - acc: 0.6293 - val_loss: 0.6189 - val_acc: 0.7192\n",
            "Epoch 29/50\n",
            "66194/66194 [==============================] - 5s 81us/step - loss: 0.6559 - acc: 0.6289 - val_loss: 0.6164 - val_acc: 0.7213\n",
            "Epoch 30/50\n",
            "66194/66194 [==============================] - 5s 81us/step - loss: 0.6522 - acc: 0.6304 - val_loss: 0.6144 - val_acc: 0.7232\n",
            "Epoch 31/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6512 - acc: 0.6325 - val_loss: 0.6131 - val_acc: 0.7246\n",
            "Epoch 32/50\n",
            "66194/66194 [==============================] - 5s 78us/step - loss: 0.6481 - acc: 0.6334 - val_loss: 0.6119 - val_acc: 0.7257\n",
            "Epoch 33/50\n",
            "66194/66194 [==============================] - 5s 73us/step - loss: 0.6470 - acc: 0.6353 - val_loss: 0.6100 - val_acc: 0.7281\n",
            "Epoch 34/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6458 - acc: 0.6366 - val_loss: 0.6071 - val_acc: 0.7309\n",
            "Epoch 35/50\n",
            "66194/66194 [==============================] - 5s 79us/step - loss: 0.6449 - acc: 0.6365 - val_loss: 0.6048 - val_acc: 0.7326\n",
            "Epoch 36/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6417 - acc: 0.6400 - val_loss: 0.6036 - val_acc: 0.7328\n",
            "Epoch 37/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6397 - acc: 0.6427 - val_loss: 0.6024 - val_acc: 0.7344\n",
            "Epoch 38/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6385 - acc: 0.6394 - val_loss: 0.6012 - val_acc: 0.7352\n",
            "Epoch 39/50\n",
            "66194/66194 [==============================] - 5s 79us/step - loss: 0.6367 - acc: 0.6420 - val_loss: 0.5996 - val_acc: 0.7364\n",
            "Epoch 40/50\n",
            "66194/66194 [==============================] - 5s 76us/step - loss: 0.6359 - acc: 0.6432 - val_loss: 0.5981 - val_acc: 0.7378\n",
            "Epoch 41/50\n",
            "66194/66194 [==============================] - 5s 77us/step - loss: 0.6352 - acc: 0.6429 - val_loss: 0.5966 - val_acc: 0.7385\n",
            "Epoch 42/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6319 - acc: 0.6460 - val_loss: 0.5957 - val_acc: 0.7393\n",
            "Epoch 43/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6300 - acc: 0.6471 - val_loss: 0.5945 - val_acc: 0.7401\n",
            "Epoch 44/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6305 - acc: 0.6485 - val_loss: 0.5937 - val_acc: 0.7406\n",
            "Epoch 45/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6269 - acc: 0.6472 - val_loss: 0.5933 - val_acc: 0.7404\n",
            "Epoch 46/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6268 - acc: 0.6466 - val_loss: 0.5918 - val_acc: 0.7416\n",
            "Epoch 47/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6265 - acc: 0.6504 - val_loss: 0.5901 - val_acc: 0.7431\n",
            "Epoch 48/50\n",
            "66194/66194 [==============================] - 5s 82us/step - loss: 0.6240 - acc: 0.6501 - val_loss: 0.5887 - val_acc: 0.7441\n",
            "Epoch 49/50\n",
            "66194/66194 [==============================] - 5s 80us/step - loss: 0.6224 - acc: 0.6496 - val_loss: 0.5877 - val_acc: 0.7451\n",
            "Epoch 50/50\n",
            "66194/66194 [==============================] - 5s 81us/step - loss: 0.6208 - acc: 0.6507 - val_loss: 0.5873 - val_acc: 0.7454\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRYenZGOGbDd",
        "colab_type": "text"
      },
      "source": [
        "We will predict the target values in our test set and compare our predictions with the real outcomes. Then we will calculate performance measures such as accuracy, precision, recall and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cOtKlOd71a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_probs = model_final.predict(X_test, verbose=0)\n",
        "yhat_classes = model_final.predict_classes(X_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iILdYO4uGcaf",
        "colab_type": "text"
      },
      "source": [
        "We need to reduce the array for predictions to 1d to be able to use it in confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8l0GXXEeIRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yhat_probs = yhat_probs[:, 0]\n",
        "yhat_classes = yhat_classes[:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQwp-BsqeNOx",
        "colab_type": "code",
        "outputId": "6122f433-93f9-42c6-c149-089b40030e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy = accuracy_score(y_test, yhat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "precision = precision_score(y_test, yhat_classes)\n",
        "print('Precision: %f' % precision)\n",
        "recall = recall_score(y_test, yhat_classes)\n",
        "print('Recall: %f' % recall)\n",
        "f1 = f1_score(y_test, yhat_classes)\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.745407\n",
            "Precision: 0.162732\n",
            "Recall: 0.506719\n",
            "F1 score: 0.246349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMZawMzHhs-",
        "colab_type": "text"
      },
      "source": [
        "Our model's accuracy is found as 0.85 which means that 85% of the target variables are correctly identified.  Precision score is found as 0.23, which means that 23% of the predicted conversions by the model actually turned out to be actual conversions by the clients. Recall score is found as 0.34, which means that our model correctly spotted 34% of all the actual conversions by the clients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Ixs0DPeTcp",
        "colab_type": "code",
        "outputId": "f06d6c2a-2120-41be-872a-cb77db09b8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "auc = roc_auc_score(y_test, yhat_probs)\n",
        "print('ROC AUC: %f' % auc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC: 0.680149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgznsbCMHNbK",
        "colab_type": "text"
      },
      "source": [
        "ROC AUC value is found as 0.68 which is higher than the benchmark value of 0.6 that is given in the task description."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IedXA-vZcA6F",
        "colab_type": "code",
        "outputId": "f1b309ce-2d9d-4f22-a0d6-c78bb76b4ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "matrix = confusion_matrix(y_test, yhat_classes)\n",
        "print(matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10842  3298]\n",
            " [  624   641]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYDJjcuGG3Z",
        "colab_type": "text"
      },
      "source": [
        "From the confusion matrix we can see that 12699 of the target variable 0s and 428 of the target variable 1s are correctly identified by the model. 1441 of the target variable 0s are incorrectly identified as 1 by the model and 837 of the target variable 1s are incorrectly identified as 0 by the model. "
      ]
    }
  ]
}